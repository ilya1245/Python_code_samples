1. Создаем S-A матрицу с равными вероятностями для всех a.
2. Проигрываем множество эмизодов, запоминаем их S, A, total_reward
3. По total_reward находим лучшие эпизоды - те, где повезло больше.
4. Создаем S-A матрицу лучших эпизодов. Вероятности действий вносим пропорционально частоте этих действий в лучших эпизодах.
5. Обновляем основную S-A матрицу используя S-A матрицу лучших эпизодов с ученом learning_rate.
6. Повторяем п.2-5 до достижения стабильного результата.