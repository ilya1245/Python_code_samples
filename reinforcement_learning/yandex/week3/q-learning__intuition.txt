1. Создаем агентна принципах q-learning, а именно:
    1.1. имеет Q(S, A) хранилище;
    1.2. умеет возвращать q(s,a);
    1.3. умеет возвращать V(s) как max(q(s,a));
    1.4. умеет обновлять q(s,a) по state, action, reward, next_state; тут происходит главная магия
        Q(s,a) := (1 - alpha) * Q(s,a) + alpha * (r + gamma * V(s'))
    1.5. умеет возвращать лучшее a(s);
    1.6. умеет возвращать a(s) как лучшее с "подмешиванием" случайных а с заданной вероятностью epsilon;

2. Проигрывание одного эпизода содержит цикл, в котором:
    2.1. получаем у агента  a(s)
    2.2. применяем это действие - env.step(a)
    2.3. обновляем q(s,a) по state, action, reward, next_state вызовом agent.update(s, a, r, next_s)
    2.4 проверяем значение среды done
    2.5. суммируем reward, сумму вернем по завершению цикла

3. Проигрываем множество эмизодов, постепенно уменьшая epsilon и мониторя return.
