{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python37764bitatariconda0122dc12f4db4a069d4ea9fafb38b87f",
      "display_name": "Python 3.7.7 64-bit ('atari': conda)"
    },
    "colab": {
      "name": "2_Modules.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BCgm0gU0VpR"
      },
      "source": [
        "# 2. Useful Modules in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kkBfWz40VpS"
      },
      "source": [
        "## Modules in PyTorch\n",
        "PyTorch provides frequently-used modules (or layers), such as FC and conv, for you to use off the shelf. <br>\n",
        "Common ways to use is as follows: <br>\n",
        "- Build module with configurations <br>\n",
        "- Call module with input, like a function. <br>\n",
        "\n",
        "In this tutorial, we will cover FC, convolution, maxpool, and activation functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hvdzM8q0VpS"
      },
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xrGI2b10VpT",
        "outputId": "a4471d2c-12f2-4b69-a04c-b8e7ba9b7094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "np.random.seed(1234)\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0498f07510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0UT8trK0VpX"
      },
      "source": [
        "# Fully-connected layer\n",
        "- PyTorch provides fully-connected layer called ```nn.Linear```, which we looked at Assignment 3 and 4.\n",
        "- As we did, it takes ```in_features``` and ```out_features``` as arguments.\n",
        "- You can also speficy whether bias is used or not, using ```bias```. (Default is ```True```)\n",
        "- Input and output shape is as follows:\n",
        "    - Input: (# data, in_features)\n",
        "    - Output: (# data, out_features)\n",
        "- Refer to official document of [nn.Linear](https://pytorch.org/docs/master/generated/torch.nn.Linear.html) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LGN3ovO0VpY",
        "outputId": "80e183d0-ba60-4138-8064-6fe82e09b7a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "tensor = torch.FloatTensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
        "print(f'tensor shape: {tensor.shape}')\n",
        "\n",
        "_, in_features = tensor.shape\n",
        "out_features = 10\n",
        "\n",
        "# Define layer (module)\n",
        "fc_layer = nn.Linear(in_features=in_features, out_features=out_features, bias=True)\n",
        "\n",
        "# Pass input\n",
        "fc_layer_out = fc_layer(tensor)\n",
        "print(f'fc layer output: \\n {fc_layer_out}')\n",
        "print(f'fc layer output shape: {fc_layer_out.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor shape: torch.Size([3, 3])\n",
            "fc layer output: \n",
            " tensor([[-1.4104, -0.6703,  0.3450,  0.4731,  0.3222,  0.4481,  0.9799, -0.4291,\n",
            "          1.4523, -1.0640],\n",
            "        [-4.2138, -1.9673, -0.7159,  1.0905,  1.6210,  0.8712,  1.2267, -1.5083,\n",
            "          3.7323, -1.3673],\n",
            "        [-7.0173, -3.2643, -1.7768,  1.7079,  2.9198,  1.2942,  1.4736, -2.5876,\n",
            "          6.0123, -1.6707]], grad_fn=<AddmmBackward>)\n",
            "fc layer output shape: torch.Size([3, 10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWEH1Qe50Vpa",
        "outputId": "5b34e354-a922-45b8-b200-d0416dfb6676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "fc_weights = fc_layer.weight.data\n",
        "fc_bias = fc_layer.bias.data\n",
        "\n",
        "print(f'fc layer weights shape: \\n{fc_weights.shape}')  # out_features x in_features\n",
        "print(f'fc layer bias shape: \\n{fc_bias.shape}')        # out_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fc layer weights shape: \n",
            "torch.Size([10, 3])\n",
            "fc layer bias shape: \n",
            "torch.Size([10])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOvjQYg70Vpd"
      },
      "source": [
        "## Convolution Layer\n",
        "- PyTorch provides convolution layer called ```nn.Conv2d```, which we looked at Assignment 4.\n",
        "    - There are different kinds of convolution, but we cover ```nn.Conv2d``` only.\n",
        "- It takes ```in_channels```, ```out_channels```, ```kernel_size```, ```stride```, ```padding```, as basic arguments. (There are more, but we do not consider here.)\n",
        "- ```kernel_size``` can be integer or a tuple.\n",
        "    - If an integer is given, kernel is squared shape.\n",
        "    - If a tuple is given, kernel is of the given shape.\n",
        "    - Square kernel is common.\n",
        "- Input and output shape is as follows:\n",
        "    - Input: (# data, in_channel, in_height, in_width)\n",
        "    - Input: (# data, out_channel, out_height, out_width)\n",
        "- Refer to official document of [nn.Conv2d](https://pytorch.org/docs/master/generated/torch.nn.Conv2d.html) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XyT-bzy0Vpd"
      },
      "source": [
        "num_data = 3\n",
        "in_channels = 1\n",
        "input_height = 5\n",
        "input_width = 5\n",
        "\n",
        "conv_input = torch.rand(num_data, in_channels, input_height, input_width)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flo-O5kz0Vpg"
      },
      "source": [
        "out_channels = 3\n",
        "kernel_size = 3\n",
        "stride = 1\n",
        "padding = 1\n",
        "\n",
        "# (3, 3) squared kernel, stride 1, padding 1\n",
        "conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XqLv7hT0Vpi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJAP5R_U0Vpk",
        "outputId": "957ec9af-edfd-47dd-882d-043be21c6af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "conv_layer_out = conv_layer(conv_input)\n",
        "print(f'conv layer output shape: {conv_layer_out.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "conv layer output shape: torch.Size([3, 3, 5, 5])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk7vmAXs0Vpn"
      },
      "source": [
        "## Maxpooling layer\n",
        "\n",
        "- PyTorch provides convolution layer called ```nn.MaxPool2d```, which we looked at Assignment 4.\n",
        "    - There are different kinds of pooling, but we only cover ```nn.MaxPool2d```.\n",
        "- It takes ```kernel_size```, ```stride```, and ```padding``` as basic arguments. (There are more, but we do not consider here.)\n",
        "- ```kernel_size``` can be integer or a tuple.\n",
        "    - If an integer is given, kernel is squared shape.\n",
        "    - If a tuple is given, kernel is of the given shape.\n",
        "    - Square kernel is common.\n",
        "- Input and output shape is as follows:\n",
        "    - Input: (# data, in_channel, in_height, in_width)\n",
        "    - Input: (# data, out_channel, out_height, out_width)\n",
        "- Refer to official document of [nn.MaxPool2d](https://pytorch.org/docs/master/generated/torch.nn.MaxPool2d.html#torch.nn.MaxPool2d) for more information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-hNHnGn0Vpn",
        "outputId": "e12d463c-35ca-4da4-9ce3-fc40cfb4f7a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "maxpool_input = torch.FloatTensor([[1] * 5, [2] * 5, [3] * 5, [4] * 5, [5] * 5]).view(1, 1, 5, 5)\n",
        "maxpool_layer = nn.MaxPool2d(kernel_size=3, stride=1, padding=0)\n",
        "maxpool_out = maxpool_layer(maxpool_input)\n",
        "\n",
        "print(f'maxpool layer input: \\n{maxpool_input[0, 0]}')\n",
        "print(f'maxpool layer input shape: {maxpool_input.shape}')\n",
        "\n",
        "print(f'maxpool layer output: \\n{maxpool_out[0, 0]}')\n",
        "print(f'maxpool layer output shape: {maxpool_out.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "maxpool layer input: \n",
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [2., 2., 2., 2., 2.],\n",
            "        [3., 3., 3., 3., 3.],\n",
            "        [4., 4., 4., 4., 4.],\n",
            "        [5., 5., 5., 5., 5.]])\n",
            "maxpool layer input shape: torch.Size([1, 1, 5, 5])\n",
            "maxpool layer output: \n",
            "tensor([[3., 3., 3.],\n",
            "        [4., 4., 4.],\n",
            "        [5., 5., 5.]])\n",
            "maxpool layer output shape: torch.Size([1, 1, 3, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PWyZQ8lC0Vpq"
      },
      "source": [
        "## Activation Functions\n",
        "\n",
        "- PyTorch provides various activation functions, including sigmoid, tanh, and ReLU.\n",
        "- Activation function is also a module, so you should follow same steps: build and use like a function.\n",
        "- Here, we will cover sigmoid, tanh, and ReLU.\n",
        "- Refer to official document of [Non-linear Activations](https://pytorch.org/docs/master/nn.html#non-linear-activations-weighted-sum-nonlinearity) for more functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubLbFDu40Vpq",
        "outputId": "dd1783a6-19ca-45dc-fe1f-779806fdb402",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "sigmoid = nn.Sigmoid()\n",
        "tanh = nn.Tanh()\n",
        "relu = nn.ReLU()\n",
        "\n",
        "x = torch.randn(3, 3)\n",
        "sigmoid_out = sigmoid(x)\n",
        "tanh_out = tanh(x)\n",
        "relu_out = relu(x)\n",
        "\n",
        "print(f'Input x:\\n {x}')\n",
        "print(f'Sigmoid:\\n {sigmoid_out}')\n",
        "print(f'Tanh:\\n {tanh_out}')\n",
        "print(f'ReLU:\\n {relu_out}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input x:\n",
            " tensor([[-0.8236, -1.6671, -0.5744],\n",
            "        [-0.0760, -0.1382, -0.5599],\n",
            "        [-0.2259, -0.6567,  0.0340]])\n",
            "Sigmoid:\n",
            " tensor([[0.3050, 0.1588, 0.3602],\n",
            "        [0.4810, 0.4655, 0.3636],\n",
            "        [0.4438, 0.3415, 0.5085]])\n",
            "Tanh:\n",
            " tensor([[-0.6770, -0.9312, -0.5186],\n",
            "        [-0.0758, -0.1373, -0.5079],\n",
            "        [-0.2222, -0.5762,  0.0340]])\n",
            "ReLU:\n",
            " tensor([[0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0340]])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}